{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea02e312-6042-44a9-95a6-019a76c615ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fac567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as ddf\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64ba2b-da44-40e3-901d-0a11ac2f3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dask.delayed\n",
    "def func(hyp_dict, algorithm, df, X, iteration, clustering_on, scores_on):\n",
    "    \"\"\" Computes the scores for a given algorithm and data on a fixed set of hyperparameters. \"\"\"\n",
    "    filename = ouput_dir + f\"{algorithm}/tuning/new_{algorithm}_{iteration}_{clustering_on}_{scores_on}_{'_'.join([str(k) + '-' + str(v) for k, v in hyp_dict.items()])}.csv\"\n",
    "    scores = pd.DataFrame(columns=[\"iteration\", \"silhouette\", \"davies_bouldin\", \"calinski\"] + list(hyp_dict.keys()))\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        # determine clustering method and predict labels\n",
    "        if algorithm == \"kmeans\":\n",
    "            kmeans = KMeans(**hyp_dict).fit(df)\n",
    "            y_pred = kmeans.predict(df)\n",
    "        elif algorithm == \"ward\":\n",
    "            model = AgglomerativeClustering(**hyp_dict).fit(df)\n",
    "            y_pred = model.labels_\n",
    "        elif algorithm == \"dbscan\":\n",
    "            model = DBSCAN(**hyp_dict).fit(df)\n",
    "            y_pred = model.labels_\n",
    "        elif algorithm == \"optics\":\n",
    "            model = OPTICS(**hyp_dict).fit(df)\n",
    "            y_pred = model.labels_\n",
    "        else:\n",
    "            print(\"Clustering algorithm not implemented\")\n",
    "    \n",
    "        # compute scores\n",
    "        nnoise = sum((y_pred == -1))  # number of noise labels\n",
    "        nclusters = len(np.unique(y_pred))  # number of clusters\n",
    "        if nclusters > 1:\n",
    "            sil = silhouette_score(X, y_pred)\n",
    "            db = davies_bouldin_score(X, y_pred)\n",
    "            cal = calinski_harabasz_score(X, y_pred)\n",
    "        else:\n",
    "            sil = np.nan\n",
    "            db = np.nan\n",
    "            cal = np.nan\n",
    "    \n",
    "        # store scores\n",
    "        scores = pd.DataFrame({\"nclusters\": [nclusters], \"iteration\": [iteration], \"silhouette\": [sil], \"davies_bouldin\": [db], \n",
    "                               \"calinski\": [cal], \"clustering_on\": [clustering_on], \"scores_on\": [scores_on], \"nnoise\": [nnoise]})\n",
    "        scores.to_csv(filename, index=False)  # in case the computation does not run through entirely, we store intermediate results\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae30618-b830-4403-b4c3-959400329494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(df, compute_scores_on=None, algorithm=\"kmeans\", iterations=10, clustering_on=\"original\", scores_on=\"original\"):\n",
    "    \"\"\" Computes the scores for a given algorithm and data on various hyperparameter combinations. \"\"\"\n",
    "    if compute_scores_on is not None:\n",
    "        X = compute_scores_on.copy()\n",
    "    else:\n",
    "        X = df.copy()\n",
    "\n",
    "    hyp_dict = hyps[algorithm]\n",
    "    hyp_combinations = list(it.product(*hyp_dict.values()))\n",
    "    res = []\n",
    "    for i in range(iterations):\n",
    "        print(i)\n",
    "        for h in hyp_combinations:\n",
    "            hyp_temp = dict((b, a) for a, b in zip(h, hyp_dict.keys()))\n",
    "            res.append(func(hyp_dict=hyp_temp, algorithm=algorithm, df=df, X=X, iteration=i, clustering_on=clustering_on, scores_on=scores_on))        \n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3a6af-9eca-4885-a414-50a97ebf5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score(df, x, y, save_as=None):\n",
    "    xmap = {\"n_clusters\": \"Number of clusters\", \"eps\": \"Epsilon\", \"min_samples\": \"Minimum number of samples\"}\n",
    "    df_mean = df.dropna()[[x, y]].groupby(x).mean()\n",
    "    if y == \"silhouette\":\n",
    "        t = \"Silhouette Score\"\n",
    "        optimal_x = df_mean.idxmax().values[0]\n",
    "    elif y == \"davies_bouldin\":\n",
    "        t = \"Davies Bouldin Score\"\n",
    "        optimal_x = df_mean.idxmin().values[0]\n",
    "    elif y == \"calinski\":\n",
    "        t = \"Calinski Harabasz Score\"\n",
    "        optimal_x = df_mean.idxmax().values[0]\n",
    "\n",
    "    plt.figure()\n",
    "    sns.lineplot(df, x=x, y=y)\n",
    "    plt.ylabel(t)\n",
    "    plt.xlabel(xmap[x])\n",
    "    plt.axvline(optimal_x, color=\"orange\")\n",
    "    plt.tight_layout()\n",
    "    if save_as:\n",
    "        plt.savefig(save_as)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc9f710-3002-44c2-964c-4ff5274ede3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_scores(scores, x=\"n_clusters\", algorithm=\"kmeans\", output_path=\"output/clustering/\", save_as_suffix=None):\n",
    "    if save_as_suffix: \n",
    "        plot_score(scores, x, \"silhouette\", save_as=f\"{'silhouette'}_{algorithm}_{x}{save_as_suffix}.png\") \n",
    "        plot_score(scores, x, \"davies_bouldin\", save_as=f\"{'davies_bouldin'}_{algorithm}_{x}{save_as_suffix}.png\")  \n",
    "        plot_score(scores, x, \"calinski\", save_as=f\"{'calinski'}_{algorithm}_{x}{save_as_suffix}.png\")  \n",
    "    else:\n",
    "        plot_score(scores, x, \"silhouette\", save_as=None) \n",
    "        plot_score(scores, x, \"davies_bouldin\", save_as=None)  \n",
    "        plot_score(scores, x, \"calinski\", save_as=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72ae66-a1d7-4d89-a123-09774c7f335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_exps(df, embedding, algorithm, n_iterations=10):\n",
    "    \"\"\" For a given clustering algorithm conduct 4 experiments: \n",
    "    - clustering on original data and scores on original data \n",
    "    - clustering on original data and scores on embedding\n",
    "    - clustering on embedding data and scores on original data \n",
    "    - clustering on embedding data and scores on embedding data \n",
    "    \"\"\"\n",
    "    # define variables\n",
    "    vars = []\n",
    "    if algorithm == \"kmeans\" or algorithm == \"ward\":\n",
    "        vars = {\"n_clusters\": int}\n",
    "    elif algorithm == \"dbscan\":\n",
    "        vars = {\"eps\": float, \"min_samples\": int}\n",
    "\n",
    "    # compute clusters on embedded data, scores on embedded data ---------------------------------------------------------------------- #\n",
    "    print(\"compute clusters on embedded data, scores on embedded data\")\n",
    "    coeff_scores_ee = compute_scores(embedding, compute_scores_on=embedding, algorithm=algorithm, iterations=n_iterations, \n",
    "                                    clustering_on=\"embedding\", scores_on=\"embedding\")\n",
    "\n",
    "    # compute clusters on original data, scores on original data ---------------------------------------------------------------------- #\n",
    "    print(\"compute clusters on original data, scores on original data\")\n",
    "    coeff_scores_oo = compute_scores(df, compute_scores_on=df, algorithm=algorithm, iterations=n_iterations, \n",
    "                                     clustering_on=\"original\", scores_on=\"original\")\n",
    "   \n",
    "    # compute clusters on original data, scores on embedded data ---------------------------------------------------------------------- #\n",
    "    print(\"compute clusters on original data, scores on embedded data\")\n",
    "    coeff_scores_oe = compute_scores(df, compute_scores_on=embedding, algorithm=algorithm, iterations=n_iterations, \n",
    "                                    clustering_on=\"original\", scores_on=\"embedding\")\n",
    "\n",
    "    # compute clusters on embedded data, scores on original data ---------------------------------------------------------------------- #\n",
    "    print(\"compute clusters on embedded data, scores on original data\")\n",
    "    coeff_scores_eo = compute_scores(embedding, compute_scores_on=df, algorithm=algorithm, iterations=n_iterations, \n",
    "                                    clustering_on=\"embedding\", scores_on=\"original\")\n",
    "\n",
    "    return [coeff_scores_oo, coeff_scores_oe, coeff_scores_eo, coeff_scores_ee]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9979b705-10d8-456e-977d-9c5b06dc2000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameter combinations to evaluate\n",
    "hyps = {\"kmeans\": {\"n_clusters\": list(range(2, 16)) + [20, 30, 40, 50, 60], \"n_init\": [\"auto\"]},\n",
    "        \"ward\": {\"n_clusters\": range(2, 31), \"distance_threshold\": [None], \"linkage\": [\"ward\"]},\n",
    "        \"dbscan\": {\"eps\": np.linspace(0.01, 0.2, 60), \"min_samples\": range(2, 12)},\n",
    "        \"optics\": {\"min_samples\": range(1, 16), \"max_eps\": [np.inf]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee44ce5-6691-4636-93ef-7b10c4078b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_in = pd.read_csv(\"data/df_wide_knn.csv\") \n",
    "df = df_in.drop([\"LATITUDE\", \"LONGITUDE\", \"LEV_M\"], axis=1)  # remove geolocation\n",
    "\n",
    "# scale data\n",
    "scaler = MinMaxScaler().fit(df)\n",
    "df_scaled = pd.DataFrame(scaler.transform(df), columns=df.columns)\n",
    "\n",
    "# compute embedding\n",
    "embedding = umap.UMAP(min_dist=0.0, n_components=3, n_neighbors=20).fit_transform(df_scaled)\n",
    "\n",
    "# output directory\n",
    "output_dir = \"output_final/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f31ca4-d85c-4740-b215-e9c836319797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize embedding\n",
    "fig = plt.figure(figsize=(7, 6))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(embedding[:, 0], embedding[:, 1], embedding[:, 2], alpha=0.08, s=2, marker=\".\")\n",
    "plt.xlabel(\"Axis 0\")\n",
    "plt.ylabel(\"Axis 1\")\n",
    "ax.set_zlabel(\"Axis 2\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir + \"umap_space.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd5885-8569-4fa7-971a-b1b5335e47f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# client = Client(n_workers=4)\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a3e30-5677-4d3a-ba7d-ca9d7ec86d29",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d1544-c1d9-4c60-b927-0c557a5920c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# compute scores \n",
    "scores_kmeans = conduct_exps(df=df_scaled, embedding=embedding, algorithm=\"kmeans\", n_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7c02a-d3b9-4791-b168-49588296035e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results_kmeans = dask.compute(*[item for row in scores_kmeans for item in row])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea93ca3-bd0b-4fb3-9138-5cc23f0a3a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store results\n",
    "pd.concat(results_kmeans).to_csv(output_dir + \"kmeans_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd2742-529e-4394-a64c-bdcdb219543a",
   "metadata": {},
   "source": [
    "# Agglomerative Ward clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e288c6-6c1f-4a76-b6d7-16809f74bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores_ward = conduct_exps(df=df_scaled, embedding=embedding, algorithm=\"ward\", n_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f641588-8ea6-4b2e-be75-e5adb02fb9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results_ward = dask.compute(*scores_ward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb7d16-6cf3-4bdb-884e-de5649df4d5f",
   "metadata": {},
   "source": [
    "beginning around 9Gib per worker, peaks up to 25Gib\n",
    "\n",
    "CPU times: user 10min 15s, sys: 1min 31s, total: 11min 47s <br>\n",
    "Wall time: 6h 47min 12s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25673b1-fe65-4f20-b954-f2386684780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(results_ward).to_csv(output_dir + \"ward_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8538ee-bda7-4e77-98ac-e1911775632b",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb35e73-0bec-4fd2-9394-a2c13aab8cfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scores_dbscan = conduct_exps(df=df_scaled, embedding=embedding, algorithm=\"dbscan\", n_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c6b026-72e6-42f5-a232-d83556583e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results_dbscan = dask.compute(*scores_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0099bd-b9bc-4963-8c34-d9555ec8b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(results_dbscan).to_csv(output_dir + \"dbscan_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e3d9f-d50e-4759-aa3e-6b984d72f894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
