{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24416a6c-36fa-4034-b8d6-d4d316a75344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib as mpl\n",
    "import umap\n",
    "import utils\n",
    "import glob\n",
    "import itertools as it\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3f487a-782e-49ac-9d14-c9b27a1d7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_labels(df_train, df_store, embedding=None, iteration=0, output_dir=None, suffix=\"\"):   \n",
    "    \"\"\" Compute UMAP-DBSCAN and store clustering result. \"\"\"\n",
    "    # compute embedding\n",
    "    if embedding is None:\n",
    "        embedding = umap.UMAP(min_dist=min_dist, n_components=n_components, n_neighbors=n_neighbors).fit_transform(df_train)\n",
    "\n",
    "    # compute clustering\n",
    "    model = DBSCAN(eps=eps, min_samples=min_samples).fit(embedding)\n",
    "\n",
    "    # drop small clusters and set their label to -1\n",
    "    temp = df_store.copy()\n",
    "    temp[\"e0\"] = embedding[:, 0]\n",
    "    temp[\"e1\"] = embedding[:, 1]\n",
    "    temp[\"e2\"] = embedding[:, 2]\n",
    "    temp[\"label\"] = model.labels_\n",
    "    \n",
    "    # print(len(temp))\n",
    "    # temp, knee, thresh = utils.drop_clusters_with_few_samples(temp, thresh=None, plotting=False)\n",
    "    # print(f\"      {len(temp) -len(df_store)}\")\n",
    "\n",
    "    # store output\n",
    "    if output_dir:\n",
    "        temp.to_csv(f\"{output_dir}umap_dbscan_{iteration}{suffix}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220c2ab3-fdf2-4e29-bb57-739366703c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap_matrix(a, b):\n",
    "    \"\"\" Compute overlap matrix of two clusterings a and b, i.e. the number of grid cells each label combination from a and b have in common. \"\"\"\n",
    "    m = pd.merge(a[[\"LATITUDE\", \"LONGITUDE\", \"LEV_M\", \"label\"]], \n",
    "                 b[[\"LATITUDE\", \"LONGITUDE\", \"LEV_M\", \"label\"]], \n",
    "                 how=\"outer\", \n",
    "                 on=[\"LATITUDE\", \"LONGITUDE\", \"LEV_M\"], suffixes=[\"_a\", \"_b\"])\n",
    "\n",
    "    # count grid cells for all combinations of labels from a and b\n",
    "    counts = pd.DataFrame(m[[\"label_a\", \"label_b\"]].value_counts(dropna=False))\n",
    "    overlap_matrix = counts[\"count\"].unstack(fill_value=0).T.drop(np.nan, axis=1, errors=\"ignore\").drop(np.nan, axis=0, errors=\"ignore\")\n",
    "    \n",
    "    return overlap_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b84bcaa-c7de-4952-b084-eb2ead4c2621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap(a, b, overlap_matrix):\n",
    "    \"\"\" Compute overlap between two clusterings a and b. Also called purity. The measure is asymmetric. \"\"\"\n",
    "    N_a = len(a)\n",
    "    N_b = len(b)\n",
    "    \n",
    "    overlap_ab = 1/N_a*overlap_matrix.max(axis=0).sum()\n",
    "    overlap_ba = 1/N_b*overlap_matrix.max(axis=1).sum()\n",
    "    overlap = (overlap_ab + overlap_ba)/2\n",
    "        \n",
    "    return overlap_ab, overlap_ba, overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fc00af8-e09b-40ff-895a-de6a9a59acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f_clustering_accuracy(a, b, overlap_matrix):\n",
    "    \"\"\" Compute F_clustering_accuracy between two clusterings a and b. \"\"\"\n",
    "    N_a = len(a)\n",
    "    N_b = len(b)\n",
    "    N = (N_a + N_b)/2\n",
    "    \n",
    "    column_sum = np.array(overlap_matrix.sum(axis=0), dtype=float)\n",
    "    row_sum = np.array(overlap_matrix.sum(axis=1), dtype=float)\n",
    "    \n",
    "    divider = np.array([column_sum] * len(row_sum)) + np.array([row_sum] * len(column_sum)).T\n",
    "    factor = np.divide(1, divider, out=np.zeros(divider.shape), where=divider!=0)  # if dividing by zero, set the factor to zero instead\n",
    "    f = 2 * overlap_matrix * factor\n",
    "    \n",
    "    fca_ab = 1/N_a * np.multiply(column_sum, f.max(axis=0)).sum()\n",
    "    fca_ba = 1/N_b * np.multiply(row_sum, f.max(axis=1)).sum()\n",
    "    fca = (fca_ab + fca_ba)/2\n",
    "    \n",
    "    return fca_ab, fca_ba, fca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e600a1-d370-4436-85cd-e230abbf0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropies(a, b, overlap_matrix):\n",
    "    \"\"\" Compute entropy measures between clustering a and b, i.e. mutual information, variation of information and normalised mutual information. \"\"\"\n",
    "    N_a = len(a)\n",
    "    N_b = len(b)\n",
    "    N = (N_a + N_b)/2\n",
    "\n",
    "    # entropy of each partition\n",
    "    h_a = -1*sum(overlap_matrix.sum(axis=0)/N_a * (overlap_matrix.sum(axis=0)/N_a).map(lambda x: np.log(x) if x != 0 else 0))\n",
    "    h_b = -1*sum(overlap_matrix.sum(axis=1)/N_b * (overlap_matrix.sum(axis=1)/N_b).map(lambda x: np.log(x) if x != 0 else 0))\n",
    "    \n",
    "    # joint entropy\n",
    "    h_ab = -1*overlap_matrix.map(lambda x: x/N * np.log(x/N) if x!= 0 else 0).sum().sum()\n",
    "    \n",
    "    # entropy-related measures\n",
    "    mi = h_a + h_b - h_ab  # mutual information \n",
    "    vi = h_a + h_b - 2*mi  # variation of information\n",
    "    nmi = mi/max(h_a, h_b)  # normalised mutual information\n",
    "\n",
    "    return mi, vi, nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18af70d8-04a9-4b31-808b-6783554fa49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN hyperparameters\n",
    "eps = 0.10983051  # 0.1 \n",
    "min_samples = 4  # 3\n",
    "\n",
    "# UMAP hyperparameters\n",
    "min_dist = 0.0\n",
    "n_components = 3\n",
    "n_neighbors = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0181dfb-1ddc-4e9a-b202-1b496eabbedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output_final/dbscan/uncertainty/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "094dd823-7693-43f5-a091-edec6209571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_in = pd.read_csv(\"data/df_wide_knn.csv\") \n",
    "df = df_in.drop([\"LATITUDE\", \"LONGITUDE\", \"LEV_M\"], axis=1)  # remove geolocation\n",
    "\n",
    "# scale data\n",
    "scaler = MinMaxScaler().fit(df)\n",
    "df_scaled = pd.DataFrame(scaler.transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a25ecc-16e2-4bce-8221-2129cde06879",
   "metadata": {},
   "source": [
    "# Repeat UMAP-DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5351911b-c9f1-44e2-a752-aefde6c9bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c89b6-0c4f-4052-88a7-26c391461cfd",
   "metadata": {},
   "source": [
    "## Re-run UMAP-DBSCAN several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e0431-d9bc-482d-89fb-13eb5c6b4d25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_list = []\n",
    "\n",
    "for i in tqdm(range(num_iterations)):\n",
    "    compute_labels(df_train=df_scaled, df_store=df_in, embedding=None, iteration=i, output_dir=output_dir, suffix=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f8262-7fc9-49a5-b631-cbc4c04d1636",
   "metadata": {},
   "source": [
    "## Compute dropout and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473ecb4-3a4c-41f3-bda0-7ea16157eeed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info = []\n",
    "for filename in tqdm(glob.glob(output_dir + \"umap_dbscan_*.csv\")):\n",
    "    if not \"manualkneedrop\" in filename:\n",
    "        i = filename.split(\"_\")[3].removesuffix(\".csv\")\n",
    "        print(i)\n",
    "        \n",
    "        # load data (and compute thresh for automatic cluster-dropping)\n",
    "        filename = f\"umap_dbscan_{i}.csv\"\n",
    "        df = pd.read_csv(output_dir + filename)\n",
    "        # df_dropped, knee, thresh = utils.drop_clusters_with_few_samples(df, thresh=None, plotting=False)\n",
    "\n",
    "        # load clusterings with manually dropped data\n",
    "        filename_dropped = glob.glob(f\"{output_dir}umap_dbscan_manualkneedrop_{i}_thresh*.csv\")[0]\n",
    "        thresh = filename_dropped.split(\"thresh\")[1].rstrip(\".csv\")\n",
    "        df_dropped = pd.read_csv(filename_dropped)\n",
    "        \n",
    "        # compute scores only on valid labels\n",
    "        temp_dropped = df_dropped[df_dropped.label != -1]  \n",
    "        if len(temp_dropped[\"label\"].unique()) > 1:\n",
    "            si_dropped = silhouette_score(temp_dropped[[\"e0\", \"e1\", \"e2\"]], temp_dropped[\"label\"])\n",
    "            db_dropped = davies_bouldin_score(temp_dropped[[\"e0\", \"e1\", \"e2\"]], temp_dropped[\"label\"])\n",
    "            ch_dropped = calinski_harabasz_score(temp_dropped[[\"e0\", \"e1\", \"e2\"]], temp_dropped[\"label\"])\n",
    "        else:\n",
    "            si_dropped = np.nan\n",
    "            db_dropped = np.nan\n",
    "            ch_dropped = np.nan\n",
    "    \n",
    "        temp = df[df.label != -1]\n",
    "        if len(temp[\"label\"].unique()) > 1:\n",
    "            si = silhouette_score(temp[[\"e0\", \"e1\", \"e2\"]], temp[\"label\"])\n",
    "            db = davies_bouldin_score(temp[[\"e0\", \"e1\", \"e2\"]], temp[\"label\"])\n",
    "            ch = calinski_harabasz_score(temp[[\"e0\", \"e1\", \"e2\"]], temp[\"label\"])\n",
    "        else:\n",
    "            si = np.nan\n",
    "            db = np.nan\n",
    "            ch = np.nan\n",
    "    \n",
    "        print(f\"  #labels = {len(df['label'].unique())}\")\n",
    "        print(f\"  #labels_dropped = {len(temp_dropped['label'].unique())}\")\n",
    "    \n",
    "        # store everything\n",
    "        # temp_dropped.to_csv(f\"{output_dir}umap_dbscan_kneedrop_{i}.csv\", index=False)\n",
    "        temp_dropped_info = pd.DataFrame({\"filename\": [f\"umap_dbscan_manualkneedrop_{i}.csv\"], \n",
    "                                          \"thresh\": [thresh], \n",
    "                                          \"silhouette\": [si_dropped], \n",
    "                                          \"davies-bouldin\": [db_dropped], \n",
    "                                          \"calinski-harabasz\": [ch_dropped], \n",
    "                                          \"num_clusters\": [len(temp_dropped.label.unique())]\n",
    "                                         })\n",
    "        info.append(temp_dropped_info)\n",
    "    \n",
    "        temp_info = pd.DataFrame({\"filename\": [f\"umap_dbscan_{i}.csv\"], \n",
    "                                  \"thresh\": [None], \n",
    "                                  \"silhouette\": [si], \n",
    "                                  \"davies-bouldin\": [db], \n",
    "                                  \"calinski-harabasz\": [ch],\n",
    "                                  \"num_clusters\": [len(temp.label.unique())]\n",
    "                                 })\n",
    "        info.append(temp_info)\n",
    "\n",
    "info = pd.concat(info)\n",
    "info.to_csv(f\"{output_dir}info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3512ffff-3355-4a2c-85df-53d5d4e25103",
   "metadata": {},
   "source": [
    "## Inspect dropout and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fcf07c-513d-41fb-aaf6-f33df7922d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv(output_dir + \"info.csv\")\n",
    "info[\"dropped\"] = info.filename.apply(lambda x: True if \"kneedrop\" in x else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283a44e-9337-4372-b377-4ec86a4a37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare clusterings with and without dropping of small clusters\n",
    "l = info[~info.filename.str.contains(\"kneedrop\")]\n",
    "k = info[info.filename.str.contains(\"kneedrop\")]\n",
    "\n",
    "s0 = sns.scatterplot(data=k, x=\"silhouette\", y=\"davies-bouldin\", hue=\"thresh\")\n",
    "s1 = sns.scatterplot(data=l, x=\"silhouette\", y=\"davies-bouldin\")\n",
    "plt.show()\n",
    "\n",
    "s2 = sns.scatterplot(data=k, x=\"silhouette\", y=\"calinski-harabasz\", hue=\"thresh\")\n",
    "s3 = sns.scatterplot(data=l, x=\"silhouette\", y=\"calinski-harabasz\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(data=k, x=\"calinski-harabasz\", y=\"davies-bouldin\", hue=\"thresh\")\n",
    "sns.scatterplot(data=l, x=\"calinski-harabasz\", y=\"davies-bouldin\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(data=k, x=\"thresh\", y=\"davies-bouldin\", hue=\"silhouette\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32578e00-d8dc-49dc-a187-99bd8835379e",
   "metadata": {},
   "source": [
    "We can see that all clusterings where clusters had been removed performed better than raw clusterings (without removal).\n",
    "\n",
    "If we check the raw clusterings (below), we see a weak relation between the scores: Higher Silhouette entails lower Davis-Bouldin and higher Calinski-Harbasz, the correlation is between 0.3 and 0.5. Davis Bouldin is more correlated to the number of clusters than the others. \n",
    "\n",
    "We checked the clustering which is best according to each score (see below):\n",
    "- Calinski-Harabasz: Clustering 7\n",
    "- Davies-Bouldin: Clustering 13\n",
    "- Silhouette: Clustering 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1b2f6-6a37-4db2-9a1c-5ccd944362eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "l[['thresh', 'silhouette', 'davies-bouldin', 'calinski-harabasz', 'num_clusters']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8f6e60-7a38-4660-8979-ca44982de25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.sort_values(\"calinski-harabasz\", ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abee2e7-f044-466a-91fb-6336c0714678",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.sort_values(\"davies-bouldin\", ascending=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906489a7-0512-441f-a85e-8ebd0ad5e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.sort_values(\"silhouette\", ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf5163-1ab9-4210-a773-cd52fc7c5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(info.thresh)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec37060-51e0-43fc-b0d5-473372dfcfa3",
   "metadata": {},
   "source": [
    "## Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ea7b8-7e35-42d8-a851-fed89c3a4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute uncertainty measures\n",
    "df_res = []\n",
    "\n",
    "# iterate over each pair of clusterings\n",
    "for i in tqdm(range(num_iterations), desc=\"\"):\n",
    "    for j in range(num_iterations):\n",
    "        if j<i: \n",
    "            # load clusterings\n",
    "            clustering_a = pd.read_csv(f\"{output_dir}umap_dbscan_{i}.csv\")\n",
    "            clustering_b = pd.read_csv(f\"{output_dir}umap_dbscan_{j}.csv\")\n",
    "            \n",
    "            # ignore noise cluster\n",
    "            clustering_a = clustering_a[clustering_a.label != -1]\n",
    "            clustering_b = clustering_b[clustering_b.label != -1]\n",
    "\n",
    "            # compute overlap matrix\n",
    "            overlap_matrix = compute_overlap_matrix(a=clustering_a, b=clustering_b)\n",
    "            \n",
    "            # compute overlap\n",
    "            overlap_ab, overlap_ba, symmetric_overlap = compute_overlap(a=clustering_a, b=clustering_b, overlap_matrix=overlap_matrix)\n",
    "            fca_ab, fca_ba, fca = compute_f_clustering_accuracy(clustering_a, clustering_b, overlap_matrix)\n",
    "            mi, vi, nmi = compute_entropies(clustering_a, clustering_b, overlap_matrix)\n",
    "            df_res.append(pd.DataFrame({\"clustering_a\": [i], \"clustering_b\": [j], \n",
    "                                        \"overlap_ab\": [overlap_ab], \"overlap_ba\": [overlap_ba], \"overlap\": [symmetric_overlap],\n",
    "                                        \"f_accuracy_ab\": [fca_ab], \"f_accuracy_ba\": [fca_ba], \"f_accuracy\": [fca], \n",
    "                                        \"mutual_infomration\": [mi], \"variation_of_information\": [vi], \"normalized_mutual_information\": [nmi]\n",
    "                                       }))\n",
    "\n",
    "df_res = pd.concat(df_res)\n",
    "df_res.to_csv(output_dir + \"uncertainty_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb58f9-0d4d-4390-a8d3-b500cc998cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot uncertainties\n",
    "df_res = pd.read_csv(output_dir + \"uncertainty_scores.csv\")  # load uncertainty results\n",
    "\n",
    "sns.histplot(df_res.f_accuracy)\n",
    "plt.xlabel(\"F clustering accuracy\")\n",
    "plt.savefig(output_dir + \"f_clustering_accuracy.png\")\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(df_res.normalized_mutual_information)\n",
    "plt.xlabel(\"Normalised mutual information\")\n",
    "plt.savefig(output_dir + \"normalised_mutual_information.png\")\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(df_res.overlap)\n",
    "plt.xlabel(\"Overlap [%]\")\n",
    "plt.savefig(output_dir + \"overlap.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b786a9-6f70-4757-9636-f447dc64e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" We have a mean overlap of {df_res.overlap.mean()*100} +- {df_res.overlap.std()*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ced75-493d-4ef0-9c64-9695c3d8b618",
   "metadata": {},
   "source": [
    "## Uncertainty through cluster matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085dc67-a27d-43b1-baae-17ec50c3e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hmm maybe this is too uncertain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ae75bb-0667-41ae-89a7-2a63b28d86ce",
   "metadata": {},
   "source": [
    "### Step 1: Find maximum overlapping labels/clusters for each combination of clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6898cc3-c120-4af1-9580-40546ef1df17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compare outputs matching by #overlapping samples\n",
    "# num_iterations = len(res)\n",
    "res_matching = []\n",
    "for i in tqdm(range(num_iterations)):\n",
    "    for j in range(num_iterations):\n",
    "        if i != j:\n",
    "            # load both label sets\n",
    "            a = pd.read_csv(f\"{output_dir}umap_dbscan_{i}.csv\")\n",
    "            b = pd.read_csv(f\"{output_dir}umap_dbscan_{j}.csv\")\n",
    "            # a = pd.DataFrame(labels_list[i], columns=[\"label\"])  # pd.read_csv(output_dir + f\"eps_{eps}-min_samples_{min_samples}-iteration_{i}.csv\")\n",
    "            # b = pd.DataFrame(labels_list[j], columns=[\"label\"])  # pd.read_csv(output_dir + f\"eps_{eps}-min_samples_{min_samples}-iteration_{j}.csv\")\n",
    "\n",
    "            # compute number of clusters of both clusterings\n",
    "            num_clusters_a = len(a['label'].unique())\n",
    "            num_clusters_b = len(b['label'].unique())\n",
    "            print(f\"Number of clusters difference: {num_clusters_a - num_clusters_b}\")\n",
    "\n",
    "            # compare num samples per cluster\n",
    "            num_samples_a = pd.DataFrame(a.value_counts()).reset_index()\n",
    "            num_samples_b = pd.DataFrame(b.value_counts()).reset_index()\n",
    "\n",
    "            # match clusters by computing sample overlap\n",
    "            df_labels = a.copy()\n",
    "            df_labels = df_labels.rename(columns={\"label\": \"label_a\"})\n",
    "            df_labels = df_labels.merge(b[[\"LATITUDE\", \"LONGITUDE\", \"LEV_M\", \"label\"]].rename(columns={\"label\": \"label_b\"}), how=\"outer\", on=[\"LATITUDE\", \"LONGITUDE\", \"LEV_M\"])\n",
    "            # df_labels[\"label_a\"] = a\n",
    "            # df_labels[\"label_b\"] = b\n",
    "\n",
    "            # iterate over each cluster in a\n",
    "            for c in np.sort(a[\"label\"].unique()):\n",
    "                # check potential matches (given label c (from clusterin a), which labels in clustering b are at the same points?\n",
    "                matches = pd.DataFrame(df_labels[df_labels[\"label_a\"] == c][\"label_b\"])\n",
    "                match_counts = matches.value_counts().reset_index()\n",
    "                num_matches = len(match_counts)\n",
    "\n",
    "                # what is the cluster in b, that has most points in common with cluster c (from clustering a)?\n",
    "                max_match = match_counts[match_counts[\"count\"] == match_counts[\"count\"].max()]\n",
    "\n",
    "                # if all samples in c got assigned the same cluster in b, num_matches will be 1\n",
    "                if num_matches != 0:\n",
    "                    res_matching.append({\"clustering_a\": i, \"clustering_b\": j,\n",
    "                                \"label_a\": c,\n",
    "                                \"label_b\": max_match[\"label_b\"].values[0],\n",
    "                                \"num_samples_a\": len(matches),\n",
    "                                \"num_samples_max_match\": max_match[\"count\"].values[0],\n",
    "                                \"difference\": len(matches) - max_match[\"count\"].values[0]})\n",
    "                else:\n",
    "                    # if no matching cluster was found\n",
    "                    res_matching.append({\"clustering_a\": i, \"clustering_b\": j,\n",
    "                                \"label_a\": c,\n",
    "                                \"label_b\": np.nan,\n",
    "                                \"num_samples_a\": len(matches),\n",
    "                                \"num_samples_max_match\": np.nan,\n",
    "                                \"difference\": np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa1896-598a-4b90-a97f-c69918ad2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overlap = pd.DataFrame(res_matching)\n",
    "df_overlap.to_csv(f\"{output_dir}overlap_withoutDrop.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e03cb7-c2f7-40cb-a571-9fc63aa67953",
   "metadata": {},
   "source": [
    "### Step 2: Apply the label mapping, count the number of different labels, compute uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f6f9ae-8493-4665-aa8e-90642303b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overlap = pd.read_csv(f\"{output_dir}overlap_withoutDrop.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b09bc-8eb8-4737-88a2-739f4bfa7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb700bd5-fb69-4a4d-93aa-d1df7f79fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we always need to start from clustering a (as this is how we have done the prvious computation step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc62aa0-2f4c-42c8-9bb3-ecd558326071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uncertainty = df_in.copy()\n",
    "uncertainty[\"uncertainty\"] = 0\n",
    "\n",
    "for i in tqdm(range(num_iterations)):\n",
    "    for j in range(num_iterations):\n",
    "        if j<i: \n",
    "            df_clustering_a = pd.read_csv(f\"{output_dir}umap_dbscan_{i}.csv\").rename(columns={\"label\": \"label_a\"})\n",
    "            df_clustering_b = pd.read_csv(f\"{output_dir}umap_dbscan_{j}.csv\").rename(columns={\"label\": \"label_b\"})\n",
    "    \n",
    "            temp = df_clustering_a.copy()\n",
    "            temp = temp.merge(df_clustering_b[[\"LATITUDE\", \"LONGITUDE\", \"LEV_M\", \"label_b\"]], how=\"outer\", on=[\"LATITUDE\", \"LONGITUDE\", \"LEV_M\"])\n",
    "    \n",
    "            # define mapping between clustering a and b\n",
    "            mapping_a_b = df_overlap[(df_overlap.clustering_a == i) & (df_overlap.clustering_b == j)][[\"label_a\", \"label_b\"]]\n",
    "            # mapping_b_a = df_overlap[(df_overlap.clustering_a == j) & (df_overlap.clustering_b == i)][[\"label_a\", \"label_b\"]]\n",
    "\n",
    "            # map labes from clustering a to labels from clustering b (and vice versa)\n",
    "            # temp[\"label_a_mapped\"] = temp[\"label_a\"].apply(lambda x: mapping_a_b[mapping_a_b.label_a.astype(float).astype(str) == str(float(x))].iloc[0].label_b)  # this is the label mapping\n",
    "            # temp[\"label_b_mapped\"] = temp[\"label_b\"].apply(lambda x: mapping_b_a[mapping_b_a.label_b.astype(float).astype(str) == str(float(x))].iloc[0].label_a)  # this is the label mapping\n",
    "            temp = pd.merge(left=temp, right=mapping_a_b, how=\"left\", on=\"label_a\", suffixes=(\"\", \"_mapped\")).rename(columns={\"label_b_mapped\": \"label_a_mapped\"})\n",
    "\n",
    "            # uncertainty of clustering for given 2 clusterings\n",
    "            temp[\"are_labels_unequal_ab\"] = temp.label_a_mapped != temp.label_b\n",
    "            temp[\"cur_uncertainty\"] = temp[\"are_labels_unequal_ab\"].astype(int)\n",
    "            # temp[\"cur_uncertainty_ab\"] = temp[\"are_labels_unequal_ab\"].astype(int) \n",
    "\n",
    "            # temp[\"are_labels_unequal_ba\"] = temp.label_b_mapped != temp.label_a\n",
    "            # temp[\"cur_uncertainty_ba\"] = temp[\"are_labels_unequal_ba\"].astype(int) \n",
    "\n",
    "            # @todo in the previous step, we are just matching labels a-b (not b-a) - so we cannot compute the b-a matching here. would be nice to increase precision of our uncertainty estimate\n",
    "            # # decide which uncertainty is worse a-b or b-a and choose it (take worst case uncertainty)\n",
    "            # if temp[\"cur_uncertainty_ab\"].sum() > temp[\"cur_uncertainty_ba\"].sum():\n",
    "            #     temp[\"cur_uncertainty\"] = temp[\"cur_uncertainty_ab\"]\n",
    "            # else:\n",
    "            #     temp[\"cur_uncertainty\"] = temp[\"cur_uncertainty_ba\"]\n",
    "\n",
    "            # update overall uncertainty\n",
    "            uncertainty = uncertainty.merge(temp[[\"LATITUDE\", \"LONGITUDE\", \"LEV_M\", \"cur_uncertainty\"]], how=\"outer\", on=[\"LATITUDE\", \"LONGITUDE\", \"LEV_M\"])\n",
    "            uncertainty[\"uncertainty\"] = uncertainty[\"uncertainty\"] + uncertainty[\"cur_uncertainty\"]\n",
    "            uncertainty = uncertainty.drop(\"cur_uncertainty\", axis=1)\n",
    "\n",
    "uncertainty[\"uncertainty\"] = uncertainty[\"uncertainty\"]/(num_iterations*(num_iterations-1)/2)*100\n",
    "\n",
    "# add UMAP coordinates from last clustering\n",
    "uncertainty = pd.merge(uncertainty, df_clustering_a[[\"LATITUDE\", \"LONGITUDE\", \"LEV_M\", \"e0\", \"e1\", \"e2\"]], how=\"left\", on=[\"LATITUDE\", \"LONGITUDE\", \"LEV_M\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7898c1-4562-44c2-a818-b7a284649dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty.to_csv(f\"{output_dir}uncertainty.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f229a5-248e-4474-8ec6-c3e90767ad1b",
   "metadata": {},
   "source": [
    "### Plot matching uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333017e9-70b6-4bae-8763-9df425af75c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty = pd.read_csv(f\"{output_dir}uncertainty.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e826008-0149-4242-a17b-eab5d57067fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(uncertainty[\"uncertainty\"]) # many uncertain points...\n",
    "plt.xlabel(\"Uncertainty [%]\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}uncertainty_withDrop_histplot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073effa5-66b1-495c-8c52-e26f8688a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(2, 4))\n",
    "sns.boxplot(uncertainty[\"uncertainty\"]) # many uncertain points...\n",
    "plt.ylabel(\"Uncertainty [%]\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}uncertainty_withDrop_boxplot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae0ef5-7ef3-4a07-a467-7e0f2148c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average uncertainty: {round(uncertainty.uncertainty.mean(), 2)} +- {round(uncertainty.uncertainty.std(), 2)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe73e9-904f-4c27-952f-5290533de3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.coupled_label_plot(uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ae1f8-e4ef-4db3-a4b1-703f30c20bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Basemap\n",
    "mymap = Basemap(llcrnrlon=uncertainty[\"LONGITUDE\"].min(), llcrnrlat=uncertainty[\"LATITUDE\"].min(), \n",
    "                urcrnrlon=uncertainty[\"LONGITUDE\"].max(), urcrnrlat=uncertainty[\"LATITUDE\"].max(), fix_aspect=False)\n",
    "\n",
    "# plot\n",
    "figsize = (6, 6)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "sc_3d = ax.scatter(uncertainty[\"LONGITUDE\"], uncertainty[\"LATITUDE\"], uncertainty[\"LEV_M\"], c=uncertainty[\"uncertainty\"], s=0.5, alpha=1, zorder=4)  # df[\"predictions\"]\n",
    "ax.add_collection3d(mymap.drawcoastlines(linewidth=0.5))\n",
    "ax.set_box_aspect((np.ptp(uncertainty[\"LONGITUDE\"]), np.ptp(uncertainty[\"LATITUDE\"]), np.ptp(uncertainty[\"LEV_M\"])/50))  # aspect ratio is 1:1:1 in data space\n",
    "plt.gca().invert_zaxis()\n",
    "plt.colorbar(sc_3d, location=\"bottom\", fraction=0.05, pad=0.01, label=\"Uncertainty [%]\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}uncertainty_withDrop_geospace.png\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "sc_umap = ax.scatter(uncertainty[\"e0\"], uncertainty[\"e1\"], uncertainty[\"e2\"], c=uncertainty[\"uncertainty\"], alpha=0.8, zorder=4, s=1)  # , s=s, alpha=1, zorder=4)\n",
    "plt.colorbar(sc_umap, location=\"bottom\", fraction=0.05, pad=0.05, label=\"Uncertainty [%]\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}uncertainty_withDrop_umapspace.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3dd330-8f6c-4238-971e-9379f684da16",
   "metadata": {},
   "source": [
    "# Repeat DBSCAN (fixed UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "263a8b00-1196-4eb1-9643-1043ab2518dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3015f74-6847-4194-afdb-d89f9b19c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output_final/dbscan/uncertainty/fixed_UMAP/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f7854-bc7c-4385-be21-88bc4600b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute fixed embedding\n",
    "embedding = umap.UMAP(min_dist=min_dist, n_components=n_components, n_neighbors=n_neighbors).fit_transform(df_scaled)\n",
    "utils.plot_embedding(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65fc697-743d-457c-a085-c52dcfd983f4",
   "metadata": {},
   "source": [
    "## Re-run DBSCAN several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff600f2-5a0e-4245-9c95-39dc9234a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have training data (scaled data) and embedding in one df (needed for shuffling later on)\n",
    "df_fixedUMAP = df_scaled.copy()\n",
    "df_fixedUMAP[\"e0\"] = embedding[:, 0]\n",
    "df_fixedUMAP[\"e1\"] = embedding[:, 1]\n",
    "df_fixedUMAP[\"e2\"] = embedding[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5df93e6-5ddf-4082-9beb-bf6567623067",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = []\n",
    "\n",
    "for i in tqdm(range(num_iterations)):\n",
    "    # shuffling data (shuffling needed since DBSCAN is only sensitive to sequence of input)\n",
    "    idx = np.random.permutation(df_in.index)\n",
    "    temp_df_fixedUMAP = df_fixedUMAP.reindex(idx)\n",
    "    temp_df_in = df_in.reindex(idx)\n",
    "\n",
    "    # compute DBSCAN on given embedding\n",
    "    compute_labels(df_train=temp_df_fixedUMAP[df_scaled.columns], \n",
    "                   df_store=temp_df_in, \n",
    "                   embedding=temp_df_fixedUMAP[[\"e0\", \"e1\", \"e2\"]].to_numpy(), \n",
    "                   iteration=i, output_dir=output_dir, suffix=\"_fixedUMAP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b930a346-a9a9-4aea-9d05-1793c7dc88ce",
   "metadata": {},
   "source": [
    "## Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce70a9-edf6-46c5-b0e6-1345e787bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute uncertainty measures\n",
    "df_res = []\n",
    "\n",
    "# iterate over each pair of clusterings\n",
    "for i in tqdm(range(num_iterations), desc=\"\"):\n",
    "    for j in range(num_iterations):\n",
    "        if j<i: \n",
    "            # load clusterings\n",
    "            clustering_a = pd.read_csv(f\"{output_dir}umap_dbscan_{i}_fixedUMAP.csv\")\n",
    "            clustering_b = pd.read_csv(f\"{output_dir}umap_dbscan_{j}_fixedUMAP.csv\")\n",
    "            \n",
    "            # ignore noise cluster\n",
    "            clustering_a = clustering_a[clustering_a.label != -1]\n",
    "            clustering_b = clustering_b[clustering_b.label != -1]\n",
    "\n",
    "            # compute overlap matrix\n",
    "            overlap_matrix = compute_overlap_matrix(a=clustering_a, b=clustering_b)\n",
    "            \n",
    "            # compute overlap\n",
    "            overlap_ab, overlap_ba, symmetric_overlap = compute_overlap(a=clustering_a, b=clustering_b, overlap_matrix=overlap_matrix)\n",
    "            fca_ab, fca_ba, fca = compute_f_clustering_accuracy(clustering_a, clustering_b, overlap_matrix)\n",
    "            mi, vi, nmi = compute_entropies(clustering_a, clustering_b, overlap_matrix)\n",
    "            df_res.append(pd.DataFrame({\"clustering_a\": [i], \"clustering_b\": [j], \n",
    "                                        \"overlap_ab\": [overlap_ab], \"overlap_ba\": [overlap_ba], \"overlap\": [symmetric_overlap],\n",
    "                                        \"f_accuracy_ab\": [fca_ab], \"f_accuracy_ba\": [fca_ba], \"f_accuracy\": [fca], \n",
    "                                        \"mutual_infomration\": [mi], \"variation_of_information\": [vi], \"normalized_mutual_information\": [nmi]\n",
    "                                       }))\n",
    "\n",
    "df_res = pd.concat(df_res)\n",
    "df_res.to_csv(output_dir + \"uncertainty_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3f75f-8a80-4b59-bf31-c7eac84c1dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot uncertainties\n",
    "df_res = pd.read_csv(output_dir + \"uncertainty_scores.csv\") # load uncertainty results\n",
    "\n",
    "sns.histplot(df_res.f_accuracy)\n",
    "plt.xlabel(\"F clustering accuracy\")\n",
    "plt.savefig(output_dir + \"f_clustering_accuracy_fixedUMAP.png\")\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(df_res.normalized_mutual_information)\n",
    "plt.xlabel(\"Normalised mutual information\")\n",
    "plt.savefig(output_dir + \"normalised_mutual_information_fixedUMAP.png\")\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(df_res.overlap)\n",
    "plt.xlabel(\"Overlap [%]\")\n",
    "plt.savefig(output_dir + \"overlap_fixedUMAP.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bafb1f-29ff-496c-b863-8aaf2a8a2568",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" We have a mean overlap of {df_res.overlap.mean()*100} +- {df_res.overlap.std()*100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba8b08-f107-47d3-94ef-f563600e32f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97caaf2c-3deb-4436-bd8a-d05c17a7a285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08ab06-a622-4841-9894-cc28065302e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed17e6-e221-4932-874c-f1dda2d0f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[df_res.overlap == df_res.overlap.min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76148a4d-7b3c-49a9-bb8a-68ece2f9e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(output_dir + \"umap_dbscan_0_fixedUMAP.csv\")\n",
    "b = pd.read_csv(output_dir + \"umap_dbscan_1_fixedUMAP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2276dfeb-1632-494e-8872-33fc01bb4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = utils.color_code_labels(a)\n",
    "b = utils.color_code_labels(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf61e2f-ef00-44dd-a357-af787b4e0679",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.coupled_label_plot(a[a.label != -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c702d7ef-e736-42a4-8b59-5d00fec05a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.coupled_label_plot(b[b.label != -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749583bc-b8be-4bf0-9d06-a3ef45bd7f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591219ea-b9dc-4e5c-86d5-1e942921fb14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
